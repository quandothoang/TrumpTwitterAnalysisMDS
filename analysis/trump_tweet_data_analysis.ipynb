{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bf08c8-b3bf-4547-b58b-3095c2a40211",
   "metadata": {},
   "source": [
    "## Trump Twitter Analysis\n",
    "# Group 14: MDS 522 : Members: <br> Quan Hoang, Mailys Guegon, Joel Peterson, Li Pu \n",
    "Analyzing the <b>realDonaldTrump_in_office.csv </b>\n",
    "from https://github.com/MarkHershey/CompleteTrumpTweetsArchive/blob/master/data/realDonaldTrump_in_office.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f315d-f889-4f97-b744-f9c86355dd31",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Here we are analyzing Tweets published by Donald Trump during his first presidency, specifically how the time of day or season affect the frequency of the tweets. Additionally, we did sentiment analysis using a VADER for classification to determine the frequency of positive, negative and neutral tweets. Finally, using a combination of CountVectorizer and Logistic Regression, we used WorldCloud visualization to determine the most frequent positive and negative words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b60cd-492c-40db-a264-bc00f8493d9a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Twitter (also known as X) is one of the world's most popular social media platforms, where users can share short messages (texts, videos, photos) known as tweets.\n",
    "In April 2025, the Pew Research Center did a study and found that on average active users published 157 tweets per month, which is around 5 tweets per day. At the beginning of his first presidency, Donald Trump followed the average, posting about 5.7 times per day. However towards the second half of 2020, his posting rate grew to around 34.8 times per day. On June 5 2020, he went as far as tweeting or retweeting 200 messages in a single day. With so much data, we though it would be interesting to consider what could affect his frequency of posting. The president uses the platform mostly to disparage those he perceives as threats (people, companies, countries...), with more than half of his tweets between January 2017 and October 2019 being used to attack something or someone. Another intresting question to consider would be to determine the frequency of positive and negative tweets throughout his presidency. In this report, we will start by considering different  the factors (time of day or season) that might have an impact on the frequency of the tweets, then we will classify the tweets into positive, negative or neutral to determine the most frequent sentiment expressed in his posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33808d76-1232-44c0-bc41-15b64633d9b5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5c55ce-9926-4c11-b231-bbcf183c77c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display' from 'IPython.core.display' (/Users/coach/miniforge3/envs/522_proj/lib/python3.11/site-packages/IPython/core/display.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandera\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandera\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Column, DataFrameSchema\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtabular\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtabular\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchecks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringMismatch\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtabular\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchecks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IsSingleValue\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/522_proj/lib/python3.11/site-packages/deepchecks/__init__.py:34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalytics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manonymous_telemetry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_latest_version\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BaseCheck, BaseSuite, CheckFailure, CheckResult, Condition, ConditionCategory,\n\u001b[32m     35\u001b[39m                              ConditionResult, ModelOnlyBaseCheck, SingleDatasetBaseCheck, SuiteResult,\n\u001b[32m     36\u001b[39m                              TrainTestBaseCheck)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# TODO: remove in further versions\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtabular\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (Context, Dataset, ModelComparisonCheck, ModelComparisonSuite, ModelOnlyCheck,\n\u001b[32m     39\u001b[39m                                 SingleDatasetCheck, Suite, TrainTestCheck)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/522_proj/lib/python3.11/site-packages/deepchecks/core/__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright (C) 2021-2023 Deepchecks (https://www.deepchecks.com)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"Module for base classes.\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[33;03mImport objects to be available in parent deepchecks module.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheck_json\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CheckFailureJson, CheckResultJson\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheck_result\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CheckFailure, CheckResult\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchecks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseCheck, DatasetKind, ModelOnlyBaseCheck, SingleDatasetBaseCheck, TrainTestBaseCheck\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/522_proj/lib/python3.11/site-packages/deepchecks/core/check_json.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheck_result\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CheckFailure, CheckResult, DisplayMap\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcondition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Condition, ConditionCategory, ConditionResult\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhtml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m imagetag\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/522_proj/lib/python3.11/site-packages/deepchecks/core/check_result.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasedatatypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseFigure\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcondition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConditionCategory, ConditionResult\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DisplayableResult, save_as_html\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepchecksValueError\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreduce_classes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReduceMixin\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/522_proj/lib/python3.11/site-packages/deepchecks/core/display.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_context, process\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtempfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NamedTemporaryFile\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, display_html\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Widget\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepchecks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserialization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTMLFormatter, HtmlSerializer, IPythonSerializer, WidgetSerializer\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/Users/coach/miniforge3/envs/522_proj/lib/python3.11/site-packages/IPython/core/display.py)"
     ]
    }
   ],
   "source": [
    "import warnings # Used Chat GPT to help find a solution to hide the warning \n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated\", category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import requests\n",
    "\n",
    "# Data validation\n",
    "import pandera.pandas as pa\n",
    "from pandera.pandas import Column, DataFrameSchema\n",
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.checks import StringMismatch\n",
    "from deepchecks.tabular.checks import IsSingleValue\n",
    "import seaborn as sns\n",
    "from deepchecks.tabular.checks.data_integrity import FeatureFeatureCorrelation\n",
    "from deepchecks.tabular.checks import FeatureLabelCorrelation\n",
    "\n",
    "# Sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8d17b-379a-4a23-9448-14d35dd86192",
   "metadata": {},
   "source": [
    "## Data\n",
    "The dataset we are using contains all tweets published (includes deleted tweets) published by Donald Trump during his first presidency between 20 Jan 2017 and 08 Jan 2021. It contains 5 columns (ID, Time, Tweet URL, Tweet Text) and each row represents a tweet.\n",
    "For the initial EDA and data processing we ended up having to drop around 1/2 of the rows to stop unconventional characters at the end of the tweet string from tripping up pandas. After a visual review we noticed that these problem rows were quite evenly distributed (every 2-3 rows) and that this wouldn't be too much of an issue to get started. <br> <br>\n",
    "All of the data and rows have now been recovered. All numbers and values have been updated and applied to the analysis and put through further validation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab2a912-1026-460d-8f8a-99cc93f6ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23075, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet Text</th>\n",
       "      <th>Date &amp; Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"It all begins today! I will see you at 11:00 ...</td>\n",
       "      <td>2017-01-20 06:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Today we are not merely transferring power fr...</td>\n",
       "      <td>2017-01-20 11:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"power from Washington, D.C. and giving it bac...</td>\n",
       "      <td>2017-01-20 11:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"What truly matters is not which party control...</td>\n",
       "      <td>2017-01-20 11:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"January 20th 2017, will be remembered as the ...</td>\n",
       "      <td>2017-01-20 11:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"The forgotten men and women of our country wi...</td>\n",
       "      <td>2017-01-20 11:54:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"We will bring back our jobs. We will bring ba...</td>\n",
       "      <td>2017-01-20 11:54:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"We will follow two simple rules: BUY AMERICAN...</td>\n",
       "      <td>2017-01-20 11:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"It is time to remember that...https://www.fac...</td>\n",
       "      <td>2017-01-20 11:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"So to all Americans, in every city near and f...</td>\n",
       "      <td>2017-01-20 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Tweet Text         Date & Time\n",
       "0  \"It all begins today! I will see you at 11:00 ... 2017-01-20 06:31:00\n",
       "1  \"Today we are not merely transferring power fr... 2017-01-20 11:51:00\n",
       "2  \"power from Washington, D.C. and giving it bac... 2017-01-20 11:51:00\n",
       "3  \"What truly matters is not which party control... 2017-01-20 11:52:00\n",
       "4  \"January 20th 2017, will be remembered as the ... 2017-01-20 11:53:00\n",
       "5  \"The forgotten men and women of our country wi... 2017-01-20 11:54:00\n",
       "6  \"We will bring back our jobs. We will bring ba... 2017-01-20 11:54:00\n",
       "7  \"We will follow two simple rules: BUY AMERICAN... 2017-01-20 11:55:00\n",
       "8  \"It is time to remember that...https://www.fac... 2017-01-20 11:58:00\n",
       "9  \"So to all Americans, in every city near and f... 2017-01-20 12:00:00"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_clean_trump_csv(url):                # chatGPT assistance to construct this data-cleaning function from URL\n",
    "    resp = requests.get(url)                   # download the csv text from passed URL\n",
    "    resp.raise_for_status()                     # raise if 4xx/5xx\n",
    "    lines = resp.text.splitlines()\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i, line in enumerate(lines, start=1):\n",
    "        line = line.rstrip(\"\\n\\r\")\n",
    "        if not line.strip():\n",
    "            continue                                   # skip empty lines\n",
    "\n",
    "        parts = line.split(\",\")\n",
    "\n",
    "        if i == 1:                                        # skip header row => define custom column names later\n",
    "            continue\n",
    "\n",
    "        if len(parts) < 4:                                  # if fewer than 4 parts => it's truly broken => drop\n",
    "            continue\n",
    "\n",
    "        id_val = parts[0].strip()                              # the first 3 columns don't need cleaning\n",
    "        time_val = parts[1].strip()\n",
    "        url_val = parts[2].strip()\n",
    "        tweet_text = \",\".join(parts[3:]).strip()\n",
    "\n",
    "        rows.append((id_val, time_val, url_val, tweet_text))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"ID\", \"Time\", \"Tweet URL\", \"Tweet Text\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/MarkHershey/CompleteTrumpTweetsArchive/refs/heads/master/data/realDonaldTrump_in_office.csv\"\n",
    "\n",
    "tweets = load_clean_trump_csv(url)\n",
    "\n",
    "tweets.columns = tweets.columns.str.strip()                                # strip white-space from before column names \n",
    "#print(tweets.columns)\n",
    "tweets[\"Date & Time\"] = pd.to_datetime(tweets[\"Time\"], errors=\"coerce\")     # set Time column to DateTime and rename\n",
    "tweets = tweets.drop(columns=[\"ID\", \"Tweet URL\", \"Time\"])                     # drop ID => twitter-handle, Tweet URL, Time => now \"Date & Time\"\n",
    "\n",
    "print(tweets.shape)\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a6e72-c3ef-4dfa-b9fc-2c88468b0629",
   "metadata": {},
   "source": [
    "### DATA VALIDATION: Panderas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c56b6f-c495-49c9-8241-6205bbde6ee2",
   "metadata": {},
   "source": [
    "We have cleaned the column names to ensure they are correct and created a function to download the data from URL to ensure it has the correct file format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec1d09-bc58-4112-a078-10e449b9345d",
   "metadata": {},
   "source": [
    "### Missingness not beyond expected threshold and no empty observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4980e459-79f1-4ddf-8c71-2d25ffa9fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = DataFrameSchema(\n",
    "    {\n",
    "        \"Date & Time\": Column(pa.DateTime, nullable=False, coerce=True),\n",
    "        \"Tweet Text\": Column(pa.String, nullable=False),\n",
    "    },\n",
    "    checks=[pa.Check(lambda df: ~(df.isna().all(axis=1)).any(), error=\"Empty rows found.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "tweets_valid = schema.validate(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb099c3-de1e-4fcb-830d-adf00319feb7",
   "metadata": {},
   "source": [
    "### Correct data types in each column\n",
    "\n",
    "To ensure that our dataset is clean and ready for downstream analysis, we first verify that each column has the correct data type. Since our dataset contains text, and a timestamp index, incorrect types (such as strings for dates) could break the analysis or produce misleading results.\n",
    "Below, we inspect and correct data types as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7e7c47-4a77-420a-9825-a3eda257110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column data types:\n",
      "Tweet Text             object\n",
      "Date & Time    datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "No datetime information found (neither column nor index).\n"
     ]
    }
   ],
   "source": [
    "print(\"Column data types:\")\n",
    "print(tweets.dtypes)\n",
    "\n",
    "# Check whether datetime exists either as a column OR as index\n",
    "if isinstance(tweets.index, pd.DatetimeIndex):\n",
    "    print(\"\\nDatetime index detected — dates are correctly parsed as index.\")\n",
    "else:\n",
    "    if \"Date\" in tweets.columns:\n",
    "        print(\"\\n'Date' column exists — check first few values:\")\n",
    "        print(tweets[\"Date\"].head())\n",
    "    else:\n",
    "        print(\"\\nNo datetime information found (neither column nor index).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423959fd-e244-4704-91a5-d274f502a030",
   "metadata": {},
   "source": [
    "### No duplicates observations\n",
    "We checked for duplicated rows using DataFrame.duplicated(). The dataset contains no duplicate entries, meaning each tweet represents a unique observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8300508-7ee6-49e4-b52b-d4292072e5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 10\n",
      "Duplicates removed.\n"
     ]
    }
   ],
   "source": [
    "dup_count = tweets.duplicated().sum()\n",
    "print(f\"Number of duplicated rows: {dup_count}\")\n",
    "\n",
    "# If duplicates exist, remove them\n",
    "if dup_count > 0:\n",
    "    tweets = tweets.drop_duplicates()\n",
    "    print(\"Duplicates removed.\")\n",
    "else:\n",
    "    print(\"No duplicate observations found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e3144-060f-4cfb-a2a6-cfce2f3de3c5",
   "metadata": {},
   "source": [
    "### No outlier or anomalous values\n",
    "We examined tweet length distribution using descriptive statistics, a boxplot, and the IQR (1.5×IQR rule). 0 number of extreme values were detected, and they are reasonable for tweet text length(there's 280 characters limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128b2299-7eea-4872-bfef-596a927f4716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length descriptive statistics:\n",
      "count    23065.000000\n",
      "mean       175.421765\n",
      "std         90.695756\n",
      "min          6.000000\n",
      "25%        100.000000\n",
      "50%        168.000000\n",
      "75%        270.000000\n",
      "max        433.000000\n",
      "Name: length, dtype: float64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(tweets[\u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m].describe())\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Boxplot to visualize outliers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m sns.boxplot(data=tweets, x=\u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mOutlier Check — Tweet Length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m plt.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "tweets[\"length\"] = tweets[\"Tweet Text\"].str.len()\n",
    "\n",
    "print(\"Length descriptive statistics:\")\n",
    "print(tweets[\"length\"].describe())\n",
    "\n",
    "# Boxplot to visualize outliers\n",
    "sns.boxplot(data=tweets, x=\"length\")\n",
    "plt.title(\"Outlier Check — Tweet Length\")\n",
    "plt.show()\n",
    "\n",
    "# Using IQR rule to detect outliers\n",
    "Q1 = tweets[\"length\"].quantile(0.25)\n",
    "Q3 = tweets[\"length\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = tweets[(tweets[\"length\"] < lower_bound) | (tweets[\"length\"] > upper_bound)]\n",
    "print(f\"Number of detected outliers: {outliers.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682883cc-e6cd-4a0b-ab12-e107a7af869e",
   "metadata": {},
   "source": [
    "## Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f23a7e9-2742-42c3-965f-756878a7e036",
   "metadata": {},
   "source": [
    "### Part 1: Does the time of day/period of the year affect the frequency of the tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1d9c2-f9a4-44df-a5b4-1772ead25048",
   "metadata": {},
   "source": [
    "With the president posting 34.8 times per day it is interesting to consider whether the time of day or even the period of the year affects the frequency of his tweets. <br>\n",
    "Before our analysis, we believed that the frequency of tweets would be highest during the daytime and night-time. These assumptions were proven wrong as the numbers show that the most frequent time of day for tweets was the overnight period. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0495bed6-d628-449a-8c62-0677bf6270ea",
   "metadata": {},
   "source": [
    "-  The initial data filtering by time of day visualized in the bar chart shows that the highest frequency of tweets occurs during the evening period between 4:01 pm - 12:00 am (as mentioned above) with a total of 8825 analyzed tweets. The second most frequent tweet period occurred during the overnight period between  12:01 am - 8:00 am with a total of 7881 analyzed tweets, with the time of day resulting in the least frequent amount of tweets actually being the daytime period between 8:01am-4:00pm with a total of 6369 analyzed tweets. Perhaps this is because Trump is most busy during the day (golfing?) and cannot tend to tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b00ba0-d2fe-4f86-9c89-8612e31b11d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets in the 'Daytime' category is: 6369\n",
      "The number of tweets in the 'Evening' category is: 8823\n",
      "The number of tweets in the 'Overnight' category is: 7873\n",
      "Just by the distribution of tweets it is surprisingly evenly spread; with the most tweets happening in the evening.\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.set_index(\"Date & Time\")                                  # set index to DateTime for filtering \n",
    "\n",
    "\n",
    "daytime = tweets.between_time(\"08:01\", \"16:00\")                             # 8:01 am - 4:00 pm\n",
    "evening = tweets.between_time(\"16:01\", \"00:00\")                              # 4:01 pm - 12:00 am\n",
    "overnight = tweets.between_time(\"00:01\", \"08:00\")                             # 12:01 am - 8:00 am\n",
    "\n",
    "print(f\"The number of tweets in the 'Daytime' category is: {len(daytime)}\")\n",
    "print(f\"The number of tweets in the 'Evening' category is: {len(evening)}\")\n",
    "print(f\"The number of tweets in the 'Overnight' category is: {len(overnight)}\")\n",
    "\n",
    "print(\"Just by the distribution of tweets it is surprisingly evenly spread; with the most tweets happening in the evening.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3fae9a-1664-4dd1-8048-684c4f8dec2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daytime</td>\n",
       "      <td>6369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evening</td>\n",
       "      <td>8823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overnight</td>\n",
       "      <td>7873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time  Count\n",
       "0    daytime   6369\n",
       "1    evening   8823\n",
       "2  overnight   7873"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_time = pd.DataFrame({\n",
    "    \"Time\": [\"daytime\", \"evening\", \"overnight\"],\n",
    "    \"Count\": [len(daytime), len(evening), len(overnight)]\n",
    "})\n",
    "\n",
    "tweet_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd38b25",
   "metadata": {},
   "source": [
    "Figure 1: Number of tweets published by time of day (daytime: 8:01 am - 4:00 pm, evening: 4:01 pm - 12:00 am, overnight: 12:01 am - 8:00 am )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a094c60b-847b-489f-8c1a-d26611e5b4ce",
   "metadata": {},
   "source": [
    "-  The seasonal filtering of the tweets returned the results of the most frequent amount of tweets being in the summer with 6747 analyzed tweets, with the autumn being the second busiets season for tweets resulting in 6401 analysed tweets, followed by the spring recording 5543 analyzed tweets, and lastly the winter being the least busy season for tweets recording a number of 4384 tweets. This falls inline with inutitive assumptions about mood and activity as the summer is usually the busiest with longer days (daylight) and nicer weather, then the fall slowly tapers off into the winter season being the shortest days for daylight and the most restricting weather.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce5f133-2e5d-4f88-a3dd-9dcb56916061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets in the 'Spring' category is: 5543\n",
      "The number of tweets in the 'Summer' category is: 6744\n",
      "The number of tweets in the 'Autumn' category is: 6395\n",
      "The number of tweets in the 'Winter' category is: 4383\n",
      "\n",
      "Judging by the distribution of tweets throughout the seasons, the frequency decays in the winter and spring and peaks in the summer\n"
     ]
    }
   ],
   "source": [
    "tw = pd.Series(tweets.index.strftime('%m-%d'), index=tweets.index)     # ChatGPT assistance to troubleshoot filtering datetimes in index\n",
    "\n",
    "spring = tweets.loc[tw.between('04-01', '06-30')]                    # April 1st - June 30th\n",
    "summer = tweets.loc[tw.between('07-01', '09-30')]                   # July 1st - September 30th\n",
    "autumn = tweets.loc[tw.between('10-01', '12-31')]                  # October 1st - December 31st\n",
    "winter = tweets.loc[tw.between('01-01', '03-31')]                 # January 1st - March 31st \n",
    "\n",
    "print(f\"The number of tweets in the 'Spring' category is: {len(spring)}\")\n",
    "print(f\"The number of tweets in the 'Summer' category is: {len(summer)}\")\n",
    "print(f\"The number of tweets in the 'Autumn' category is: {len(autumn)}\")\n",
    "print(f\"The number of tweets in the 'Winter' category is: {len(winter)}\")\n",
    "print(\"\")\n",
    "print(\"Judging by the distribution of tweets throughout the seasons, the frequency decays in the winter and spring and peaks in the summer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb828332-3184-46d6-9955-32bc388a7ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spring</td>\n",
       "      <td>5543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summer</td>\n",
       "      <td>6744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autumn</td>\n",
       "      <td>6395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Winter</td>\n",
       "      <td>4383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  Count\n",
       "0  Spring   5543\n",
       "1  Summer   6744\n",
       "2  Autumn   6395\n",
       "3  Winter   4383"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_season = pd.DataFrame({\n",
    "    \"Season\": [\"Spring\", \"Summer\", \"Autumn\", \"Winter\"],\n",
    "    \"Count\": [len(spring), len(summer), len(autumn), len(winter)]\n",
    "})\n",
    "\n",
    "tweet_season"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3cf12",
   "metadata": {},
   "source": [
    "Figure 2: Number of tweets published by season (Spring: April 1st - June 30th, Summer: July 1st - September 30th, Autumn: October 1st - December 31st, Winter: January 1st - March 31st)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7d3e3-b57c-4775-add2-09781184db1a",
   "metadata": {},
   "source": [
    "Creating the time of day frequency charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82514047-e016-431b-882c-e221177226d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bars = alt.Chart(tweet_time).mark_bar(color=\"#f00808\").encode(\n",
    "    y = \"Time:N\", \n",
    "    x = \"Count:Q\",\n",
    "    ).properties(\n",
    "    title=\"Trump's Tweet Frequency by Time of Day\",\n",
    "    width=500,\n",
    "    height=350)\n",
    "\n",
    "#time_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3e738a-5dd0-46d5-8362-a8aba0276195",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame({\n",
    "    \"Time\": [\"daytime\", \"evening\", \"overnight\"],\n",
    "    \"range\": [\"8:01am–4:00pm\", \"4:01pm–12:00am\", \"12:01am–8:00am\"],\n",
    "    \"Count\": tweet_time[\"Count\"].values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52bef1f8-f939-44ce-b7fd-387fc311647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_text = (\n",
    "    alt.Chart(label_df)\n",
    "    .mark_text(\n",
    "        align=\"center\",\n",
    "        baseline=\"middle\",\n",
    "        color=\"white\",\n",
    "        fontSize=16,\n",
    "        dx=-80\n",
    "    )\n",
    "    .encode(\n",
    "        y=\"Time:N\",\n",
    "        x=\"Count:Q\",\n",
    "        text=\"range:N\"\n",
    "    )\n",
    ")\n",
    "\n",
    "#time_bars + range_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "748efe05-3760-471b-b87e-ef9476525374",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_text = alt.Chart(label_df).mark_text(\n",
    "    align=\"left\",\n",
    "    baseline=\"middle\",\n",
    "    dx=5,\n",
    "    color=\"black\",\n",
    "    fontSize=14\n",
    ").encode(\n",
    "    y=\"Time:N\",\n",
    "    x=\"Count:Q\",      \n",
    "    text=\"Count:Q\"\n",
    ")\n",
    "\n",
    "tweet_times = time_bars + range_text + count_text\n",
    "#tweet_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8d9c7bc-2bc0-4e26-bd0b-ea7f7a260266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-4aa4d86dcc8f4cfb8869e1b21495f635.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-4aa4d86dcc8f4cfb8869e1b21495f635.vega-embed details,\n",
       "  #altair-viz-4aa4d86dcc8f4cfb8869e1b21495f635.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-4aa4d86dcc8f4cfb8869e1b21495f635\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-4aa4d86dcc8f4cfb8869e1b21495f635\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-4aa4d86dcc8f4cfb8869e1b21495f635\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-c9293147c521a02005e95422613e697d\"}, \"mark\": {\"type\": \"bar\", \"color\": \"#f00808\"}, \"encoding\": {\"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Time\", \"type\": \"nominal\"}}, \"title\": \"Trump's Tweet Frequency by Time of Day\"}, {\"data\": {\"name\": \"data-a1be5e946d989fcae0405eb4287d0904\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"color\": \"white\", \"dx\": -80, \"fontSize\": 16}, \"encoding\": {\"text\": {\"field\": \"range\", \"type\": \"nominal\"}, \"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Time\", \"type\": \"nominal\"}}}, {\"data\": {\"name\": \"data-a1be5e946d989fcae0405eb4287d0904\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"color\": \"black\", \"dx\": 5, \"fontSize\": 14}, \"encoding\": {\"text\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Time\", \"type\": \"nominal\"}}}], \"height\": 350, \"padding\": {\"right\": 50, \"top\": 15, \"bottom\": 15, \"left\": 10}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-c9293147c521a02005e95422613e697d\": [{\"Time\": \"daytime\", \"Count\": 6369}, {\"Time\": \"evening\", \"Count\": 8823}, {\"Time\": \"overnight\", \"Count\": 7873}], \"data-a1be5e946d989fcae0405eb4287d0904\": [{\"Time\": \"daytime\", \"range\": \"8:01am\\u20134:00pm\", \"Count\": 6369}, {\"Time\": \"evening\", \"range\": \"4:01pm\\u201312:00am\", \"Count\": 8823}, {\"Time\": \"overnight\", \"range\": \"12:01am\\u20138:00am\", \"Count\": 7873}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_time_of_day_chart = (time_bars + range_text + count_text).properties(padding={\"right\": 50, \"top\":15, \"bottom\":15, \"left\":10})   # <= add some white-space to balance appearance\n",
    "final_time_of_day_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa361b1",
   "metadata": {},
   "source": [
    "Figure 3: Trump's Tweet Frequency by Time of Day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976c823-1e23-4a0f-aa28-71fce07eac85",
   "metadata": {},
   "source": [
    "Creating the seasonal frequency charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "734d3172-a31e-40d2-95cf-f805fc3c352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_bars = alt.Chart(tweet_season).mark_bar(color=\"#f00808\").encode(\n",
    "    x=\"Count:Q\",\n",
    "    y=alt.Y(\"Season:N\",sort=[\"Spring\", \"Summer\", \"Autumn\", \"Winter\"])\n",
    "    ).properties(\n",
    "    title=\"Trump's Tweet Frequency by Season\",\n",
    "    width=500,\n",
    "    height=350)\n",
    "\n",
    "#season_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab859e0d-5a06-4843-a134-50f084d9b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_label_df = pd.DataFrame({\n",
    "    \"Season\": [\"Spring\", \"Summer\", \"Autumn\", \"Winter\"],\n",
    "    \"range\": [\"April 1st - June 30th\", \"July 1st – Sept 30th\", \"Oct 1st – Dec 31st\", \"Jan 1st - March 31st\"],\n",
    "    \"Count\": tweet_season[\"Count\"].values\n",
    "})\n",
    "#season_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6126fe9c-75d0-4460-82bf-ea82f2522a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_range_text = alt.Chart(season_label_df).mark_text(\n",
    "        align=\"center\",\n",
    "        baseline=\"middle\",\n",
    "        color=\"white\",\n",
    "        fontSize=16,\n",
    "        dx=-90\n",
    "    ).encode(\n",
    "        y=alt.Y(\"Season:N\",sort=[\"Spring\", \"Summer\", \"Autumn\", \"Winter\"]),\n",
    "        x=\"Count:Q\",\n",
    "        text=\"range:N\"\n",
    "    )\n",
    "\n",
    "#season_bars + season_range_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c2c7c8-982e-4978-8d30-bf6c56e5167c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'season_label_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m season_count_text = alt.Chart(\u001b[43mseason_label_df\u001b[49m).mark_text(\n\u001b[32m      2\u001b[39m     align=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     baseline=\u001b[33m\"\u001b[39m\u001b[33mmiddle\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     dx=\u001b[32m5\u001b[39m,\n\u001b[32m      5\u001b[39m     color=\u001b[33m\"\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     fontSize=\u001b[32m14\u001b[39m\n\u001b[32m      7\u001b[39m ).encode(\n\u001b[32m      8\u001b[39m     y=alt.Y(\u001b[33m\"\u001b[39m\u001b[33mSeason:N\u001b[39m\u001b[33m\"\u001b[39m,sort=[\u001b[33m\"\u001b[39m\u001b[33mSpring\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSummer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAutumn\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWinter\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m      9\u001b[39m     x=\u001b[33m\"\u001b[39m\u001b[33mCount:Q\u001b[39m\u001b[33m\"\u001b[39m,      \n\u001b[32m     10\u001b[39m     text=\u001b[33m\"\u001b[39m\u001b[33mCount:Q\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m tweet_seasons = season_bars + season_range_text + season_count_text\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#tweet_seasons\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'season_label_df' is not defined"
     ]
    }
   ],
   "source": [
    "season_count_text = alt.Chart(season_label_df).mark_text(\n",
    "    align=\"left\",\n",
    "    baseline=\"middle\",\n",
    "    dx=5,\n",
    "    color=\"black\",\n",
    "    fontSize=14\n",
    ").encode(\n",
    "    y=alt.Y(\"Season:N\",sort=[\"Spring\", \"Summer\", \"Autumn\", \"Winter\"]),\n",
    "    x=\"Count:Q\",      \n",
    "    text=\"Count:Q\"\n",
    ")\n",
    "\n",
    "tweet_seasons = season_bars + season_range_text + season_count_text\n",
    "#tweet_seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "090f746e-cffd-4bf0-9cd7-19c468337f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-a1a49ec0268240fa89f4eff09b89b0d0.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-a1a49ec0268240fa89f4eff09b89b0d0.vega-embed details,\n",
       "  #altair-viz-a1a49ec0268240fa89f4eff09b89b0d0.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-a1a49ec0268240fa89f4eff09b89b0d0\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-a1a49ec0268240fa89f4eff09b89b0d0\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-a1a49ec0268240fa89f4eff09b89b0d0\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-d5e71f6798c300e3861f0a7aa80f69e3\"}, \"mark\": {\"type\": \"bar\", \"color\": \"#f00808\"}, \"encoding\": {\"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Season\", \"sort\": [\"Spring\", \"Summer\", \"Autumn\", \"Winter\"], \"type\": \"nominal\"}}, \"title\": \"Trump's Tweet Frequency by Season\"}, {\"data\": {\"name\": \"data-0cde272f572a05af2135bb1473705dad\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"color\": \"white\", \"dx\": -90, \"fontSize\": 16}, \"encoding\": {\"text\": {\"field\": \"range\", \"type\": \"nominal\"}, \"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Season\", \"sort\": [\"Spring\", \"Summer\", \"Autumn\", \"Winter\"], \"type\": \"nominal\"}}}, {\"data\": {\"name\": \"data-0cde272f572a05af2135bb1473705dad\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"color\": \"black\", \"dx\": 5, \"fontSize\": 14}, \"encoding\": {\"text\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Season\", \"sort\": [\"Spring\", \"Summer\", \"Autumn\", \"Winter\"], \"type\": \"nominal\"}}}], \"height\": 350, \"padding\": {\"right\": 50, \"top\": 15, \"bottom\": 15, \"left\": 10}, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-d5e71f6798c300e3861f0a7aa80f69e3\": [{\"Season\": \"Spring\", \"Count\": 5543}, {\"Season\": \"Summer\", \"Count\": 6744}, {\"Season\": \"Autumn\", \"Count\": 6395}, {\"Season\": \"Winter\", \"Count\": 4383}], \"data-0cde272f572a05af2135bb1473705dad\": [{\"Season\": \"Spring\", \"range\": \"April 1st - June 30th\", \"Count\": 5543}, {\"Season\": \"Summer\", \"range\": \"July 1st \\u2013 Sept 30th\", \"Count\": 6744}, {\"Season\": \"Autumn\", \"range\": \"Oct 1st \\u2013 Dec 31st\", \"Count\": 6395}, {\"Season\": \"Winter\", \"range\": \"Jan 1st - March 31st\", \"Count\": 4383}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_season_chart = (season_bars + season_range_text + season_count_text).properties(padding={\"right\": 50, \"top\":15, \"bottom\":15, \"left\":10})  # aesthetic padding \n",
    "\n",
    "final_season_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b6bf0",
   "metadata": {},
   "source": [
    "Figure 4: Trump's Tweet Frequency by Season\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffadbd64-9f2b-4da4-88c8-e8e14dea90d0",
   "metadata": {},
   "source": [
    "Visualizing our results we see that there is no major difference in posting frequency given the time of day, but there is a difference for different seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5496aa66-4bb8-4fb3-b074-1e5d8922a985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-47bb1a98538e41dc9d29faad9eb5b341.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-47bb1a98538e41dc9d29faad9eb5b341.vega-embed details,\n",
       "  #altair-viz-47bb1a98538e41dc9d29faad9eb5b341.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-47bb1a98538e41dc9d29faad9eb5b341\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-47bb1a98538e41dc9d29faad9eb5b341\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-47bb1a98538e41dc9d29faad9eb5b341\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"layer\": [{\"data\": {\"name\": \"data-d5e71f6798c300e3861f0a7aa80f69e3\"}, \"mark\": {\"type\": \"bar\", \"color\": \"#f00808\"}, \"encoding\": {\"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Season\", \"sort\": [\"Spring\", \"Summer\", \"Autumn\", \"Winter\"], \"type\": \"nominal\"}}, \"title\": \"Trump's Tweet Frequency by Season\"}, {\"data\": {\"name\": \"data-0cde272f572a05af2135bb1473705dad\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"color\": \"white\", \"dx\": -90, \"fontSize\": 16}, \"encoding\": {\"text\": {\"field\": \"range\", \"type\": \"nominal\"}, \"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Season\", \"sort\": [\"Spring\", \"Summer\", \"Autumn\", \"Winter\"], \"type\": \"nominal\"}}}, {\"data\": {\"name\": \"data-0cde272f572a05af2135bb1473705dad\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"color\": \"black\", \"dx\": 5, \"fontSize\": 14}, \"encoding\": {\"text\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Season\", \"sort\": [\"Spring\", \"Summer\", \"Autumn\", \"Winter\"], \"type\": \"nominal\"}}}], \"height\": 350, \"width\": 500}, {\"layer\": [{\"data\": {\"name\": \"data-c9293147c521a02005e95422613e697d\"}, \"mark\": {\"type\": \"bar\", \"color\": \"#f00808\"}, \"encoding\": {\"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Time\", \"type\": \"nominal\"}}, \"title\": \"Trump's Tweet Frequency by Time of Day\"}, {\"data\": {\"name\": \"data-a1be5e946d989fcae0405eb4287d0904\"}, \"mark\": {\"type\": \"text\", \"align\": \"center\", \"baseline\": \"middle\", \"color\": \"white\", \"dx\": -80, \"fontSize\": 16}, \"encoding\": {\"text\": {\"field\": \"range\", \"type\": \"nominal\"}, \"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Time\", \"type\": \"nominal\"}}}, {\"data\": {\"name\": \"data-a1be5e946d989fcae0405eb4287d0904\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"color\": \"black\", \"dx\": 5, \"fontSize\": 14}, \"encoding\": {\"text\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Time\", \"type\": \"nominal\"}}}], \"height\": 350, \"width\": 500}], \"padding\": {\"right\": 50, \"top\": 25, \"bottom\": 25, \"left\": 25}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-d5e71f6798c300e3861f0a7aa80f69e3\": [{\"Season\": \"Spring\", \"Count\": 5543}, {\"Season\": \"Summer\", \"Count\": 6744}, {\"Season\": \"Autumn\", \"Count\": 6395}, {\"Season\": \"Winter\", \"Count\": 4383}], \"data-0cde272f572a05af2135bb1473705dad\": [{\"Season\": \"Spring\", \"range\": \"April 1st - June 30th\", \"Count\": 5543}, {\"Season\": \"Summer\", \"range\": \"July 1st \\u2013 Sept 30th\", \"Count\": 6744}, {\"Season\": \"Autumn\", \"range\": \"Oct 1st \\u2013 Dec 31st\", \"Count\": 6395}, {\"Season\": \"Winter\", \"range\": \"Jan 1st - March 31st\", \"Count\": 4383}], \"data-c9293147c521a02005e95422613e697d\": [{\"Time\": \"daytime\", \"Count\": 6369}, {\"Time\": \"evening\", \"Count\": 8823}, {\"Time\": \"overnight\", \"Count\": 7873}], \"data-a1be5e946d989fcae0405eb4287d0904\": [{\"Time\": \"daytime\", \"range\": \"8:01am\\u20134:00pm\", \"Count\": 6369}, {\"Time\": \"evening\", \"range\": \"4:01pm\\u201312:00am\", \"Count\": 8823}, {\"Time\": \"overnight\", \"range\": \"12:01am\\u20138:00am\", \"Count\": 7873}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_charts = (tweet_seasons | tweet_times).properties(padding={\"right\": 50, \"top\":25, \"bottom\":25, \"left\":25}) \n",
    "tweet_charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9724f710",
   "metadata": {},
   "source": [
    "Figure 5: Trump's Tweet Frequency by Time of Day and Season \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02be83-b8d7-4b5d-8ef2-21e84b5d3b1c",
   "metadata": {},
   "source": [
    "### Part 2: How many tweets are positive vs negative? (2 classification methods)\n",
    "When we think of the tweets posted by the president, we tend to think mostly of those where he critices or attacks others. However, is this a real representation of the sentiments of his tweets? In this part we will use two different sentiment analysis models to determine the frequency of positive, negative and neutral tweets.\n",
    "\n",
    "<b> Method 1:</b>  \n",
    "We use a simple sentiment analysis model (VADER) to classify each tweet as positive, negative, or neutral, and then compare the counts, this methodological inspiration is from ChatGPT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe938b1d-2b5d-477b-902a-f5f6a6570ba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentimentIntensityAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sia = SentimentIntensityAnalyzer()\n",
      "\u001b[31mNameError\u001b[39m: name 'SentimentIntensityAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54798c5c-768c-4c9f-af8f-f1eea7b78e4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m tweets_q2 = tweets.copy()\n\u001b[32m      2\u001b[39m tweets_q2[\u001b[33m\"\u001b[39m\u001b[33mTweet Text\u001b[39m\u001b[33m\"\u001b[39m] = tweets_q2[\u001b[33m\"\u001b[39m\u001b[33mTweet Text\u001b[39m\u001b[33m\"\u001b[39m].fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tweets_q2[\u001b[33m\"\u001b[39m\u001b[33msentiment_score\u001b[39m\u001b[33m\"\u001b[39m] = tweets_q2[\u001b[33m\"\u001b[39m\u001b[33mTweet Text\u001b[39m\u001b[33m\"\u001b[39m].apply(\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m t: sia.polarity_scores(t)[\u001b[33m\"\u001b[39m\u001b[33mcompound\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore_to_label\u001b[39m(score, pos_threshold=\u001b[32m0.05\u001b[39m, neg_threshold=-\u001b[32m0.05\u001b[39m):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m score >= pos_threshold:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[32m   4937\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4938\u001b[39m         func,\n\u001b[32m   4939\u001b[39m         convert_dtype=convert_dtype,\n\u001b[32m   4940\u001b[39m         by_row=by_row,\n\u001b[32m   4941\u001b[39m         args=args,\n\u001b[32m   4942\u001b[39m         kwargs=kwargs,\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m     ).apply()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_standard()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = obj._map_values(\n\u001b[32m   1503\u001b[39m     mapper=curried, na_action=action, convert=\u001b[38;5;28mself\u001b[39m.convert_dtype\n\u001b[32m   1504\u001b[39m )\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer(values, mapper, convert=convert)\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m      1\u001b[39m tweets_q2 = tweets.copy()\n\u001b[32m      2\u001b[39m tweets_q2[\u001b[33m\"\u001b[39m\u001b[33mTweet Text\u001b[39m\u001b[33m\"\u001b[39m] = tweets_q2[\u001b[33m\"\u001b[39m\u001b[33mTweet Text\u001b[39m\u001b[33m\"\u001b[39m].fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m      4\u001b[39m tweets_q2[\u001b[33m\"\u001b[39m\u001b[33msentiment_score\u001b[39m\u001b[33m\"\u001b[39m] = tweets_q2[\u001b[33m\"\u001b[39m\u001b[33mTweet Text\u001b[39m\u001b[33m\"\u001b[39m].apply(\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m t: sia.polarity_scores(t)[\u001b[33m\"\u001b[39m\u001b[33mcompound\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore_to_label\u001b[39m(score, pos_threshold=\u001b[32m0.05\u001b[39m, neg_threshold=-\u001b[32m0.05\u001b[39m):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m score >= pos_threshold:\n",
      "\u001b[31mNameError\u001b[39m: name 'sia' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_q2 = tweets.copy()\n",
    "tweets_q2[\"Tweet Text\"] = tweets_q2[\"Tweet Text\"].fillna(\"\").astype(str)\n",
    "\n",
    "tweets_q2[\"sentiment_score\"] = tweets_q2[\"Tweet Text\"].apply(\n",
    "    lambda t: sia.polarity_scores(t)[\"compound\"]\n",
    ")\n",
    "\n",
    "def score_to_label(score, pos_threshold=0.05, neg_threshold=-0.05):\n",
    "    if score >= pos_threshold:\n",
    "        return \"positive\"\n",
    "    elif score <= neg_threshold:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "tweets_q2[\"sentiment_label\"] = tweets_q2[\"sentiment_score\"].apply(score_to_label)\n",
    "\n",
    "tweets_q2[[\"Tweet Text\", \"sentiment_score\", \"sentiment_label\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b5d6ca1-4dc3-4660-a594-961d720492bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentiment_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m sentiment_counts = (\n\u001b[32m      2\u001b[39m     tweets_q2\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m       .groupby(\u001b[33m\"\u001b[39m\u001b[33msentiment_label\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m       .size()\n\u001b[32m      5\u001b[39m       .reset_index(name=\u001b[33m\"\u001b[39m\u001b[33mCount\u001b[39m\u001b[33m\"\u001b[39m)   \n\u001b[32m      6\u001b[39m       .rename(columns={\u001b[33m\"\u001b[39m\u001b[33msentiment_label\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mSentiment\u001b[39m\u001b[33m\"\u001b[39m})  \n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m sentiment_counts\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/frame.py:9210\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[32m   9211\u001b[39m     obj=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m   9212\u001b[39m     keys=by,\n\u001b[32m   9213\u001b[39m     axis=axis,\n\u001b[32m   9214\u001b[39m     level=level,\n\u001b[32m   9215\u001b[39m     as_index=as_index,\n\u001b[32m   9216\u001b[39m     sort=sort,\n\u001b[32m   9217\u001b[39m     group_keys=group_keys,\n\u001b[32m   9218\u001b[39m     observed=observed,\n\u001b[32m   9219\u001b[39m     dropna=dropna,\n\u001b[32m   9220\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1331\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1328\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m     grouper, exclusions, obj = get_grouper(\n\u001b[32m   1332\u001b[39m         obj,\n\u001b[32m   1333\u001b[39m         keys,\n\u001b[32m   1334\u001b[39m         axis=axis,\n\u001b[32m   1335\u001b[39m         level=level,\n\u001b[32m   1336\u001b[39m         sort=sort,\n\u001b[32m   1337\u001b[39m         observed=\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default \u001b[38;5;28;01melse\u001b[39;00m observed,\n\u001b[32m   1338\u001b[39m         dropna=\u001b[38;5;28mself\u001b[39m.dropna,\n\u001b[32m   1339\u001b[39m     )\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/groupby/grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'sentiment_label'"
     ]
    }
   ],
   "source": [
    "\n",
    "sentiment_counts = (\n",
    "    tweets_q2\n",
    "      .groupby(\"sentiment_label\")\n",
    "      .size()\n",
    "      .reset_index(name=\"Count\")   \n",
    "      .rename(columns={\"sentiment_label\": \"Sentiment\"})  \n",
    ")\n",
    "\n",
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669e061",
   "metadata": {},
   "source": [
    "Figure 6: Number of tweets per sentiment (negative, neutral, positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afce697-59d5-4b85-ae19-32e0ca99217f",
   "metadata": {},
   "source": [
    "Using the VADER sentiment analyzer the tweets were classified as either positive, neutral, or negative. At first, we believed that the tweets were likely more negative than positive, but in fact the tweets were classified as far more positive than negative, almost doubling the negatively classified tweets. The tweets classified as 'neutral' fall almost directly in between the tweets classified as positive and negative. The actual context and literal meaning of the tweets was not analyzed, nor the accuracy of the classification at this point, so there is certainly going to be a margin of error and some 'false positives' etc as the positivity is quite often a self directed compliment of sorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "970d1d4f-71a4-4820-a5c0-8c0a1ff58065",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sentiment_chart = (\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     alt.Chart(sentiment_counts)\n\u001b[32m      3\u001b[39m     .mark_bar()\n\u001b[32m      4\u001b[39m     .encode(\n\u001b[32m      5\u001b[39m         x=alt.X(\u001b[33m\"\u001b[39m\u001b[33mSentiment:N\u001b[39m\u001b[33m\"\u001b[39m, sort=[\u001b[33m\"\u001b[39m\u001b[33mpositive\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mneutral\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnegative\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m      6\u001b[39m         y=alt.Y(\u001b[33m\"\u001b[39m\u001b[33mCount:Q\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m         tooltip=[\u001b[33m\"\u001b[39m\u001b[33mSentiment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCount\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m     .properties(\n\u001b[32m     10\u001b[39m         title=\u001b[33m\"\u001b[39m\u001b[33mNumber of Positive, Neutral, and Negative Tweets\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m sentiment_chart\n",
      "\u001b[31mNameError\u001b[39m: name 'sentiment_counts' is not defined"
     ]
    }
   ],
   "source": [
    "sentiment_chart = (\n",
    "    alt.Chart(sentiment_counts)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=alt.X(\"Sentiment:N\", sort=[\"positive\", \"neutral\", \"negative\"]),\n",
    "        y=alt.Y(\"Count:Q\"),\n",
    "        tooltip=[\"Sentiment\", \"Count\"]\n",
    "    )\n",
    "    .properties(\n",
    "        title=\"Number of Positive, Neutral, and Negative Tweets\"\n",
    "    )\n",
    ")\n",
    "\n",
    "sentiment_chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec861baf",
   "metadata": {},
   "source": [
    "Figure 7: Chart Comparing the Number of Positive, Neutral, and Negative Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b546e-feae-4701-a927-9bab362e5843",
   "metadata": {},
   "source": [
    "<b> Method 2:</b>   \n",
    "Using a CountVectorizer and Logistic Regression, combining with a Wordcloud visualization to determine the most frequent words in the positive and negative tweets. Syntax and bug fixing credited to ChatGPT 5.1 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de4c0906-5ad7-40af-8266-f23c93747edb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# ambiguous, skip from training\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 2. Create weak labels for a subset of tweets\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m tweets[\u001b[33m\"\u001b[39m\u001b[33mweak_label\u001b[39m\u001b[33m\"\u001b[39m] = tweets[\u001b[33m\"\u001b[39m\u001b[33mTweet Text\u001b[39m\u001b[33m\"\u001b[39m].apply(weak_label)\n\u001b[32m     40\u001b[39m train_df = tweets.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mweak_label\u001b[39m\u001b[33m\"\u001b[39m]).copy()\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumber of weak labeled tweets:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_df))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[32m   4937\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4938\u001b[39m         func,\n\u001b[32m   4939\u001b[39m         convert_dtype=convert_dtype,\n\u001b[32m   4940\u001b[39m         by_row=by_row,\n\u001b[32m   4941\u001b[39m         args=args,\n\u001b[32m   4942\u001b[39m         kwargs=kwargs,\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m     ).apply()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_standard()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = obj._map_values(\n\u001b[32m   1503\u001b[39m     mapper=curried, na_action=action, convert=\u001b[38;5;28mself\u001b[39m.convert_dtype\n\u001b[32m   1504\u001b[39m )\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer(values, mapper, convert=convert)\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mweak_label\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m tokens = simple_tokenize(text)\n\u001b[32m     26\u001b[39m pos_hits = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m positive_words)\n\u001b[32m     27\u001b[39m neg_hits = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m negative_words)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36msimple_tokenize\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Lowercase everything, remove URLs and non letters, then split.\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m text = text.lower()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttp\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS+\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, text)          \u001b[38;5;66;03m# remove URLs\u001b[39;00m\n\u001b[32m     18\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[^a-z\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, text)        \u001b[38;5;66;03m# keep letters and spaces\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text.split()\n",
      "\u001b[31mNameError\u001b[39m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "positive_words = set(\"\"\"good great amazing fantastic tremendous strong win winning beautiful success successful\n",
    "happy proud respect love best positive incredible honored grateful huge strong strongest great big\"\"\".split())\n",
    "\n",
    "negative_words = set(\"\"\"bad terrible horrible weak fail failure disaster sad angry corrupt\n",
    "worst negative unfair hate disgrace stupid dishonest democrat biden obama democrats sleepy joe illegal\"\"\".split())  # democrat, biden, obama usually associated negatively when Trump speaks\n",
    "\n",
    "stopwords = set(\"\"\"the a an and of to in is it this that for on with be as by are was were will from at have has but not or if so\n",
    "you your my our their they we i he she his her him them rt s all t just now amp more very about do what who people word should m realdonaldtrump u\"\"\".split())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Helper functions\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    \"\"\"Lowercase everything, remove URLs and non letters, then split.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)          # remove URLs\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)        # keep letters and spaces\n",
    "    return text.split()\n",
    "\n",
    "def weak_label(text: str):\n",
    "    \"\"\"Use positive and negative lexicons to create weak labels for training.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    tokens = simple_tokenize(text)\n",
    "    pos_hits = sum(1 for w in tokens if w in positive_words)\n",
    "    neg_hits = sum(1 for w in tokens if w in negative_words)\n",
    "    if pos_hits > neg_hits:\n",
    "        return \"positive\"\n",
    "    elif neg_hits > pos_hits:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return None  # ambiguous, skip from training\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Create weak labels for a subset of tweets\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "tweets[\"weak_label\"] = tweets[\"Tweet Text\"].apply(weak_label)\n",
    "train_df = tweets.dropna(subset=[\"weak_label\"]).copy()\n",
    "\n",
    "print(\"Number of weak labeled tweets:\", len(train_df))\n",
    "print(train_df[\"weak_label\"].value_counts())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Train a simple ML model using CountVectorizer\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[\"Tweet Text\"],\n",
    "    train_df[\"weak_label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"weak_label\"]\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=list(stopwords), min_df=3)\n",
    "X_train_vec = vectorizer.fit_transform(X_train.fillna(\"\"))\n",
    "X_test_vec = vectorizer.transform(X_test.fillna(\"\"))\n",
    "\n",
    "clf = LogisticRegression(max_iter=2000)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"Validation accuracy (weak labels):\", clf.score(X_test_vec, y_test))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Predict sentiment for all tweets (positive vs negative)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "all_X = vectorizer.transform(tweets[\"Tweet Text\"].fillna(\"\"))\n",
    "tweets[\"Sentiment\"] = clf.predict(all_X)\n",
    "\n",
    "print(tweets[\"Sentiment\"].value_counts())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Answer the question \"What are the most frequent words in the positive and negative tweets?\"\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "pos_text = \" \".join(tweets[tweets[\"Sentiment\"] == \"positive\"][\"Tweet Text\"].dropna())\n",
    "neg_text = \" \".join(tweets[tweets[\"Sentiment\"] == \"negative\"][\"Tweet Text\"].dropna())\n",
    "\n",
    "pos_tokens = [w for w in simple_tokenize(pos_text) if w not in stopwords]\n",
    "neg_tokens = [w for w in simple_tokenize(neg_text) if w not in stopwords]\n",
    "\n",
    "top_pos = Counter(pos_tokens).most_common(20)\n",
    "top_neg = Counter(neg_tokens).most_common(20)\n",
    "\n",
    "top_pos_df = pd.DataFrame(top_pos, columns=[\"word\", \"count\"])\n",
    "top_neg_df = pd.DataFrame(top_neg, columns=[\"word\", \"count\"])\n",
    "\n",
    "print(\"\\nMost frequent words in positive tweets:\")\n",
    "print(top_pos_df)\n",
    "\n",
    "print(\"\\nMost frequent words in negative tweets:\")\n",
    "print(top_neg_df)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. Wordcloud\n",
    "# -------------------------------------------------------------------\n",
    "pos_wc = WordCloud(\n",
    "    width=900,\n",
    "    height=500,\n",
    "    background_color=\"white\"\n",
    ").generate(\" \".join(pos_tokens))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(pos_wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Positive Tweets Word Cloud\")\n",
    "plt.show()\n",
    "\n",
    "neg_wc = WordCloud(\n",
    "    width=900,\n",
    "    height=500,\n",
    "    background_color=\"white\"\n",
    ").generate(\" \".join(neg_tokens))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(neg_wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Negative Tweets Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2f22e",
   "metadata": {},
   "source": [
    "Figure 8a: Table with the number of weak labeled tweets <br>\n",
    "Figure 8b: Table with the number of tweets per sentiment (positive or negative) <br>\n",
    "Figure 8c: Table of the most frequent words used in positive tweets <br>\n",
    "Figure 8d: Word cloud of the positive tweets <br>\n",
    "Figure 8e: Word cloud of the negative tweets <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38995a-df4a-47a9-b79c-7c6f92ba072d",
   "metadata": {},
   "source": [
    "The second classifier method used to analyze the tweets was utilizing the CountVectorizer() to classify sets as positive words, negative words and some stop words unique to this dataset. The overall positive VS negative tweet counts are skewed in the opposite direction as the 'neutral' category here is somewhat added to the postively classified category. The results of the most frequent words are not very surprising to anyone who has followed American news at all in the last 10 years with the positive tweets containing words of praise and common phrases used by Trump, as well as his own name. The most frequent words appearing in the negative tweets could also be somewhat assumed as they contain words of the opposing political parties and leaders as well as other common phrases and words that one would hear often in a news report of Trump lashing out over twitter or a speech. A somewhat interesting note is that the word 'Trump' appears frequently in both positive and negative classifications, but could be rationalized by the fact that Trump often speaks about himself in the third person. A great visual indicator of these word frequencies is the word clouds for the respective most frequent positive and negatively classified words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cbafc-6bf1-48eb-b2e9-801d42ef5b28",
   "metadata": {},
   "source": [
    "### DATA VALIDATION: Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e762863-f4d5-43b5-b80b-962d580a8b06",
   "metadata": {},
   "source": [
    "#### Correct category levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a1da5-ba65-44f5-8589-9721ed2ff904",
   "metadata": {},
   "source": [
    "##### Checking for string mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ea86fac-4058-4d6a-a175-8dbc4b72d1f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "DeepchecksValueError",
     "evalue": "label column Sentiment not found in dataset columns",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDeepchecksValueError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset = Dataset(tweets.reset_index().drop(\u001b[33m'\u001b[39m\u001b[33mTweet Text\u001b[39m\u001b[33m'\u001b[39m,axis=\u001b[32m1\u001b[39m), label = \u001b[33m'\u001b[39m\u001b[33mSentiment\u001b[39m\u001b[33m'\u001b[39m, cat_features=[\u001b[33m'\u001b[39m\u001b[33mweak_label\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      2\u001b[39m result = StringMismatch().run(dataset)\n\u001b[32m      3\u001b[39m result.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/deepchecks/tabular/dataset.py:169\u001b[39m, in \u001b[36mDataset.__init__\u001b[39m\u001b[34m(self, df, label, features, cat_features, index_name, set_index_from_dataframe_index, datetime_name, set_datetime_from_dataframe_index, convert_datetime, datetime_args, max_categorical_ratio, max_categories, label_type, dataset_name, label_classes)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(label, Hashable):\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data.columns:\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m DeepchecksValueError(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlabel column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in dataset columns\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mself\u001b[39m._label_name = label\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mDeepchecksValueError\u001b[39m: label column Sentiment not found in dataset columns"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(tweets.reset_index().drop('Tweet Text',axis=1), label = 'Sentiment', cat_features=['weak_label'])\n",
    "result = StringMismatch().run(dataset)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590e186-50a4-44a8-b3bb-5048f859a1a7",
   "metadata": {},
   "source": [
    "##### Checking for single values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64bc5c5c-adb2-4f95-919c-9307d1d39a80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IsSingleValue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sv = IsSingleValue()\n\u001b[32m      2\u001b[39m sv.run(dataset)\n",
      "\u001b[31mNameError\u001b[39m: name 'IsSingleValue' is not defined"
     ]
    }
   ],
   "source": [
    "sv = IsSingleValue()\n",
    "sv.run(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a221e9-eaaa-4972-b179-32fb6ad72f5b",
   "metadata": {},
   "source": [
    "#### Target/response variable follows expected distribution validation: \n",
    "For this question, since the dataset we used did not have a \"target\" column, we had to try using different methods in order to find the \"sentiment\" of the tweets (this has been discussed with Instructor Sky Sheng and she gave us the go ahead. As the graphs/values of the check for usiing CountVectorizer and Logistic Regression below suggest, we do see a skewness towards positive tweets, which can be surprising to many people. This is partly due to neutral tweets are lumped together with positive tweets. However, since this dataset is about Trump's tweets while he was in office, those tweets could be positive because he was the \"powerful man\" back then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0ebd2d3-aab3-4663-a078-351db8feebae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Data validation related to target\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------------------\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. Distribution of Sentiment Graph\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m sns.countplot(data=tweets, x=\u001b[33m\"\u001b[39m\u001b[33mSentiment\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mDistribution of Sentiment Labels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m plt.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Data validation related to target\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Distribution of Sentiment Graph\n",
    "# -------------------------------------------------------------------\n",
    "sns.countplot(data=tweets, x=\"Sentiment\")\n",
    "plt.title(\"Distribution of Sentiment Labels\")\n",
    "plt.show()\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Distribution of Sentiment Proportion\n",
    "# -------------------------------------------------------------------\n",
    "print(tweets[\"Sentiment\"].value_counts(normalize=True))\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Time-based Distribution of Sentiment Graph\n",
    "# -------------------------------------------------------------------\n",
    "tweets.resample(\"M\")[\"Sentiment\"].value_counts().unstack().plot(kind=\"bar\", figsize=(12,5))\n",
    "plt.title(\"Sentiment over time (monthly)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60b003-b062-4770-a414-9e99fc99c7ea",
   "metadata": {},
   "source": [
    "Figure 9a: Graph with the distribution of sentiment labels <br>\n",
    "Figure 9b: Proportion of sentiment labels values <br>\n",
    "Figure 9c: Graph with the distribution of sentiment labels over time <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91a283-766a-4194-bd9a-50afc92dc9df",
   "metadata": {},
   "source": [
    "### DATA VALIDATION: Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c832c79-77ae-4058-ac9a-9cf207a4973e",
   "metadata": {},
   "source": [
    "#### No anomalous correlations between target/response variable and features/explanatory variables validation: \n",
    "For this question, again since the dataset we used did not have a \"target\" column, we tried our best to validate what was done. We can see that negative tweets has a higher mean length compared to positive tweets. However, there is no real correlation between time of day (even though negative tweets seem to appear slightly later in the day) or day of the week. In the wordclouds above, we do not see a clear intersection of words both appearing frequently in positive tweets and negative tweets which shows that our classification is working quite well. Last but not least, the model probability distribution graph shows that the model confidently picked up positive and negative tweets while still showing uncertainty for ambiguous cases, which is expected for a simple classifier trained on weak, lexicon-based labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3729f-bd4f-43da-a794-ff38eacfa31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Data validation related to target pt.2\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Numeric correlations\n",
    "# -------------------------------------------------------------------\n",
    "tweets[\"length\"] = tweets[\"Tweet Text\"].str.len()\n",
    "print(tweets.groupby(\"Sentiment\")[\"length\"].describe())\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Categorical correlations (hour of day, weekday)\n",
    "# -------------------------------------------------------------------\n",
    "tweets[\"hour\"] = tweets.index.hour\n",
    "tweets[\"weekday\"] = tweets.index.dayofweek\n",
    "sns.boxplot(data=tweets, x=\"Sentiment\", y=\"hour\")\n",
    "plt.show()\n",
    "sns.boxplot(data=tweets, x=\"Sentiment\", y=\"weekday\")\n",
    "plt.show()\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Model probability checks\n",
    "# -------------------------------------------------------------------\n",
    "probs = clf.predict_proba(vectorizer.transform(tweets[\"Tweet Text\"]))\n",
    "tweets[\"pos_prob\"] = probs[:, 1]\n",
    "sns.histplot(tweets[\"pos_prob\"])\n",
    "plt.title(\"Prediction Probability Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df92251b-a210-4c13-8d64-28bf81f74d08",
   "metadata": {},
   "source": [
    "Figure 10a: Tweet length stats according to sentiments <br>\n",
    "Figure 10b: Distribution graphs for time of day and day of the week related to sentiments <br>\n",
    "Figure 10c: Model prediction probability distribution graph <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe04e4c",
   "metadata": {},
   "source": [
    "#### No anomalous correlations between features/explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd85a3f7-766d-4627-9aff-d6baa79323d8",
   "metadata": {},
   "source": [
    "Since our data set only contains a date time feature and a text feature, to check the correlation between them using deepchecks we can turn these features into categorical and numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7d148-4a3f-42e0-ba7c-9b0efe17b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions to turn Date & Time into categorical and numerical features\n",
    "def season(date):\n",
    "    if 4 <= date.month <=6:\n",
    "        return 'spring'\n",
    "    elif 7 <= date.month <=9:\n",
    "        return 'spring'\n",
    "    elif 10 <= date.month <=12:\n",
    "        return 'autumn'\n",
    "    else:\n",
    "        return 'winter'\n",
    "\n",
    "def daytime(date):\n",
    "    if pd.Timestamp('08:01').time() <= date.time() <= pd.Timestamp('16:00').time():\n",
    "        return 'daytime'\n",
    "    elif pd.Timestamp('16:01').time() <= date.time() <= pd.Timestamp('00:00').time():\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'overnight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f63e8-5fbc-4d5c-a360-b13baa9888a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions to turn Tweet Text into categorical and numerical features\n",
    "def avg_word_length(text):\n",
    "    average = 0\n",
    "    for word in text.split() :\n",
    "        average += len(word)\n",
    "    return round(average/len(text.split()),1)\n",
    "\n",
    "def punctuation_count(text):\n",
    "    count = 0\n",
    "    for char in text:\n",
    "        if not char.isalnum() and not char.isspace():\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2952f-107f-47ec-84bf-1adf6eef0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the numerical and categorical features \n",
    "cat_feature_tweets = tweets.reset_index()\n",
    "cat_feature_tweets['year']=cat_feature_tweets['Date & Time'].dt.year # create year feature\n",
    "cat_feature_tweets['month']=cat_feature_tweets['Date & Time'].dt.month # create month feature\n",
    "cat_feature_tweets['day']=cat_feature_tweets['Date & Time'].dt.day # create day feature\n",
    "cat_feature_tweets['season']=cat_feature_tweets['Date & Time'].apply(season) # create season feature\n",
    "cat_feature_tweets['time_of_day']=cat_feature_tweets['Date & Time'].apply(daytime) # create time of day feature\n",
    "cat_feature_tweets['avg_word_length']=cat_feature_tweets['Tweet Text'].apply(avg_word_length) # create average word length feature\n",
    "cat_feature_tweets['word_count']=cat_feature_tweets['Tweet Text'].apply(lambda x:len(x.split())) # create number of words feature\n",
    "cat_feature_tweets['punctuation_count']=cat_feature_tweets['Tweet Text'].apply(punctuation_count) # create number of punctuation feature\n",
    "cat_feature_tweets = cat_feature_tweets.drop(columns=['weak_label','Date & Time','Tweet Text', 'pos_prob','hour'])\n",
    "cat_feature_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8facd809-8128-4de5-9f1a-8cc3335af091",
   "metadata": {},
   "source": [
    "<b> Checking feature/feature correlation of using Deepchecks <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7103da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_ds = Dataset(cat_feature_tweets,label = 'Sentiment', cat_features=['season','time_of_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7c627-8291-4656-a70c-c7202fe692df",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_feat_feat_corr = FeatureFeatureCorrelation().add_condition_max_number_of_pairs_above_threshold(threshold = 0.95, n_pairs = 1)\n",
    "check_feat_feat_corr_result = check_feat_feat_corr.run(dataset=cat_feature_ds)\n",
    "\n",
    "if not check_feat_feat_corr_result.passed_conditions():\n",
    "    raise ValueError(\"Feature-feature correlation exceeds the maximum acceptable threshold.\")\n",
    "\n",
    "# ValueError works as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5274515-321e-48e1-a6fb-f01b49bec291",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Within this report we considered multiple aspects of the tweets. Firstly we determined that Donald Trump's most productive time and season for posting was overnight during the summer. Then we determined that, contrary to what we originally believed, most of his tweets are actually positively connotated. Now, as mentioned above, our sentiment classification methods might not be a faithful representation of the sentiment of the tweets, as the models do not consider intricacies (sarcasm, neutral tweets added to positive class) that can change the sentiment of the message. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe138a63-210c-4109-ac18-57de6aea4349",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Gottfried, J., Park, E., & Nolan, H. (n.d.). Americans’ Social Media Use 2025 Growing shares of U.S. adults say they are using Instagram, TikTok, WhatsApp and Reddit, but YouTube still rises to the top FOR MEDIA OR OTHER INQUIRIES. Retrieved November 21, 2025, from [https://www.pewresearch.org/wp-content/uploads/sites/20/2025/11/PI_2025.11.20_Social-Media-Use_REPORT.pdf](https://www.pewresearch.org/wp-content/uploads/sites/20/2025/11/PI_2025.11.20_Social-Media-Use_REPORT.pdf) <br><br>\n",
    "McCarthy, N. (2021, January 11). Infographic: End Of The Road For Trump’s Twitter Account. Statista Daily Data; Statista. [https://www.statista.com/chart/19561/total-number-of-tweets-from-donald-trump/?srsltid=AfmBOorYyvrCIBJxWCwAxW5yYEl6cPXzdhu-oMfRfAPfoXrcdpIEA3fy](https://www.statista.com/chart/19561/total-number-of-tweets-from-donald-trump/?srsltid=AfmBOorYyvrCIBJxWCwAxW5yYEl6cPXzdhu-oMfRfAPfoXrcdpIEA3fy)<br><br>\n",
    "Mythili Sampathkumar. (2018, January 17). The tweets that have defined Donald Trump’s presidency | The Independent. The Independent. [https://www.independent.co.uk/news/world/americas/us-politics/donald-trump-twitter-president-first-year-a8163791.html](https://www.independent.co.uk/news/world/americas/us-politics/donald-trump-twitter-president-first-year-a8163791.html) <br><br>\n",
    "Shear, M. D., Haberman, M., Confessore, N., Yourish, K., Buchanan, L., & Collins, K. (2019, November 2). How Trump Reshaped the Presidency in Over 11,000 Tweets. The New York Times. [https://www.nytimes.com/interactive/2019/11/02/us/politics/trump-twitter-presidency.html](https://www.nytimes.com/interactive/2019/11/02/us/politics/trump-twitter-presidency.html) <br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
